{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae154e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e3b455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\gabriel\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\gabriel\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f835a2",
   "metadata": {},
   "source": [
    "## Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aba0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"teste\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(header='true', inferSchema='true', delimiter=\",\")\n",
    "    .load(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\olist_customers_dataset.csv\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414f3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|            SP|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|                    9790|            SP|\n",
      "|4e7b3e00288586ebd...|060e732b5b29e8181...|                    1151|            SP|\n",
      "|b2b6027bc5c5109e5...|259dac757896d24d7...|                    8775|            SP|\n",
      "|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            SP|\n",
      "|879864dab9bc30475...|4c93744516667ad3b...|                   89254|            SC|\n",
      "|fd826e7cf63160e53...|addec96d2e059c80c...|                    4534|            SP|\n",
      "|5e274e7a0c3809e14...|57b2a98a409812fe9...|                   35182|            MG|\n",
      "|5adf08e34b2e99398...|1175e95fb47ddff9d...|                   81560|            PR|\n",
      "|4b7139f34592b3a31...|9afe194fb833f79e3...|                   30575|            MG|\n",
      "|9fb35e4ed6f0a14a4...|2a7745e1ed516b289...|                   39400|            MG|\n",
      "|5aa9e4fdd4dfd2095...|2a46fb94aef5cbeeb...|                   20231|            RJ|\n",
      "|b2d1536598b73a9ab...|918dc87cd72cd9f6e...|                   18682|            SP|\n",
      "|eabebad39a88bb6f5...|295c05e81917928d7...|                    5704|            SP|\n",
      "|1f1c7bf1c9b041b29...|3151a81801c838636...|                   95110|            RS|\n",
      "|206f3129c0e4d7d0b...|21f748a16f4e1688a...|                   13412|            SP|\n",
      "|a7c125a0a07b75146...|5c2991dbd08bbf3cf...|                   22750|            RJ|\n",
      "|c5c61596a3b6bd0ce...|b6e99561fe6f34a55...|                    7124|            SP|\n",
      "|9b8ce803689b3562d...|7f3a72e8f988c6e73...|                    5416|            SP|\n",
      "|49d0ea0986edde72d...|3e6fd6b2f0d499456...|                   68485|            PA|\n",
      "+--------------------+--------------------+------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data:\n",
    "df = spark.read.parquet('C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\')\n",
    "\n",
    "# Clean data:\n",
    "df = df.na.drop()\n",
    "\n",
    "# Select important features from data\n",
    "columns = [\n",
    "    \"customer_id\",\n",
    "    \"customer_unique_id\",\n",
    "    \"customer_zip_code_prefix\",\n",
    "    \"customer_state\"\n",
    "]\n",
    "\n",
    "customers = df.select(*columns)\n",
    "\n",
    "(\n",
    "    customers\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\customers\\\\\")\n",
    ")\n",
    "\n",
    "customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bdadf",
   "metadata": {},
   "source": [
    "## Order Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66509f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_sequential: integer (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: integer (nullable = true)\n",
      " |-- payment_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"teste\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(header='true', inferSchema='true', delimiter=\",\")\n",
    "    .load(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\olist_order_payments_dataset.csv\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1b57f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+\n",
      "|            order_id|payment_type|payment_value|\n",
      "+--------------------+------------+-------------+\n",
      "|b81ef226f3fe1789b...| credit_card|        99.33|\n",
      "|a9810da82917af2d9...| credit_card|        24.39|\n",
      "|25e8ea4e93396b6fa...| credit_card|        65.71|\n",
      "|ba78997921bbcdc13...| credit_card|       107.78|\n",
      "|42fdf880ba16b47b5...| credit_card|       128.45|\n",
      "|298fcdf1f73eb413e...| credit_card|        96.12|\n",
      "|771ee386b001f0620...| credit_card|        81.16|\n",
      "|3d7239c394a212faa...| credit_card|        51.84|\n",
      "|1f78449c87a54faf9...| credit_card|       341.09|\n",
      "|0573b5e23cbd79800...|      boleto|        51.95|\n",
      "|d88e0d5fa41661ce0...| credit_card|       188.73|\n",
      "|2480f727e869fdeb3...| credit_card|        141.9|\n",
      "|616105c9352a9668c...| credit_card|        75.78|\n",
      "|cf95215a722f3ebf2...| credit_card|       102.66|\n",
      "|769214176682788a9...| credit_card|       105.28|\n",
      "|12e5cfe0e4716b59a...| credit_card|       157.45|\n",
      "|61059985a6fc0ad64...| credit_card|       132.04|\n",
      "|79da3f5fe31ad1e45...| credit_card|        98.94|\n",
      "|8ac09207f415d55ac...| credit_card|       244.15|\n",
      "|b2349a3f20dfbeef6...| credit_card|       136.71|\n",
      "+--------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data:\n",
    "df = spark.read.parquet('C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\')\n",
    "\n",
    "# Clean data:\n",
    "df = df.na.drop()\n",
    "\n",
    "# Select important features from data\n",
    "columns = [\n",
    "    \"order_id\",\n",
    "    \"payment_type\",\n",
    "    \"payment_value\"\n",
    "]\n",
    "payments = df.select(*columns)\n",
    "\n",
    "(\n",
    "    payments\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\payments\\\\\")\n",
    ")\n",
    "\n",
    "payments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca57cf",
   "metadata": {},
   "source": [
    "## Order Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f8e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"teste\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(header='true', inferSchema='true', delimiter=\",\")\n",
    "    .load(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\olist_order_items_dataset.csv\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8bf984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-------------+------------------+\n",
      "|            order_id|          product_id| price|freight_value|       price_total|\n",
      "+--------------------+--------------------+------+-------------+------------------+\n",
      "|00010242fe8c5a6d1...|4244733e06e7ecb49...|  58.9|        13.29|             72.19|\n",
      "|00018f77f2f0320c5...|e5f2d52b802189ee6...| 239.9|        19.93|            259.83|\n",
      "|000229ec398224ef6...|c777355d18b72b67a...| 199.0|        17.87|            216.87|\n",
      "|00024acbcdf0a6daa...|7634da152a4610f15...| 12.99|        12.79|             25.78|\n",
      "|00042b26cf59d7ce6...|ac6c3623068f30de0...| 199.9|        18.14|218.04000000000002|\n",
      "|00048cc3ae777c65d...|ef92defde845ab845...|  21.9|        12.69|34.589999999999996|\n",
      "|00054e8431b9d7675...|8d4f2bb7e93e6710a...|  19.9|        11.85|             31.75|\n",
      "|000576fe39319847c...|557d850972a7d6f79...| 810.0|        70.75|            880.75|\n",
      "|0005a1a1728c9d785...|310ae3c140ff94b03...|145.95|        11.65|             157.6|\n",
      "|0005f50442cb953dc...|4535b0e1091c278df...| 53.99|         11.4|             65.39|\n",
      "|00061f2a7bc09da83...|d63c1011f49d98b97...| 59.99|         8.88|             68.87|\n",
      "|00063b381e2406b52...|f177554ea93259a5b...|  45.0|        12.98|57.980000000000004|\n",
      "|0006ec9db01a64e59...|99a4788cb24856965...|  74.0|        23.32|             97.32|\n",
      "|0008288aa423d2a3f...|368c6c730842d7801...|  49.9|        13.37|63.269999999999996|\n",
      "|0008288aa423d2a3f...|368c6c730842d7801...|  49.9|        13.37|63.269999999999996|\n",
      "|0009792311464db53...|8cab8abac59158715...|  99.9|        27.65|127.55000000000001|\n",
      "|0009c9a17f916a706...|3f27ac8e699df3d30...| 639.0|        11.34|            650.34|\n",
      "|000aed2e25dbad2f9...|4fa33915031a8cde0...| 144.0|         8.77|            152.77|\n",
      "|000c3e6612759851c...|b50c950aba0dcead2...|  99.0|        13.71|112.71000000000001|\n",
      "|000e562887b1f2006...|5ed9eaf534f6936b5...|  25.0|        16.11|             41.11|\n",
      "+--------------------+--------------------+------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data:\n",
    "df = spark.read.parquet('C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\')\n",
    "\n",
    "# Clean data:\n",
    "df = df.na.drop()\n",
    "\n",
    "# Select important features from data\n",
    "columns = [\n",
    "    \"order_id\",\n",
    "    \"product_id\",\n",
    "    \"price\",\n",
    "    \"freight_value\"\n",
    "]\n",
    "df = df.select(*columns)\n",
    "\n",
    "# Total Price:\n",
    "items = df.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"price\"),\n",
    "    col(\"freight_value\"),\n",
    "    ((col(\"price\") + col(\"freight_value\"))).alias(\"price_total\")\n",
    ")\n",
    "\n",
    "(\n",
    "    items\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\items\\\\\")\n",
    ")\n",
    "\n",
    "items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d256b1",
   "metadata": {},
   "source": [
    "## Order Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8dd62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- review_score: string (nullable = true)\n",
      " |-- review_comment_title: string (nullable = true)\n",
      " |-- review_comment_message: string (nullable = true)\n",
      " |-- review_creation_date: string (nullable = true)\n",
      " |-- review_answer_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"teste\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(header='true', inferSchema='true', delimiter=\",\")\n",
    "    .load(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\olist_order_reviews_dataset.csv\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e7db0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|           review_id|            order_id|review_score|\n",
      "+--------------------+--------------------+------------+\n",
      "|8670d52e15e00043a...|b9bf720beb4ab3728...|           4|\n",
      "|3948b09f7c818e2d8...|e51478e7e277a8374...|           5|\n",
      "|373cbeecea8286a2b...|583174fbe37d3d5f0...|           1|\n",
      "|d21bbc789670eab77...|4fc44d78867142c62...|           5|\n",
      "|c92cdd7dd544a01aa...|37e7875cdce5a9e5b...|           4|\n",
      "|08c9d79ec0eba1d25...|e029f708df3cc108b...|           5|\n",
      "|b193ff3c9f32a01f3...|e2e6ee1ed2d7f2f36...|           5|\n",
      "|86c5cfa7fcbde303f...|a6456e781cb962cc3...|           5|\n",
      "|500c05500aa275953...|8a9424899aac432d8...|           5|\n",
      "|109b5ce2dd11bb846...|25362fbf6aac4b01a...|           5|\n",
      "|c45811d9f90e22a81...|491f193fc52075598...|           5|\n",
      "|50a1eaa2f96d6f3e0...|4a7cf245701068d38...|           5|\n",
      "|1692078634b63c7f2...|5bc4e94aef2841f39...|           5|\n",
      "|46d8249ea59101c72...|f25ddb6cd62d720a5...|           3|\n",
      "|79927442ebcbf70b2...|1c8898140458c37fb...|           5|\n",
      "|5f938e5f5f2e9a757...|d9ff0185a30043540...|           1|\n",
      "|0c9b6ca9b3beaf0fd...|cadd4c5765abf6581...|           5|\n",
      "|59d95e0b5b52d038d...|fc8c2e8bc069aea36...|           5|\n",
      "|c40a6b6e0181e5ec0...|f3468d6a2c7586711...|           1|\n",
      "|5241fcad10d7b45a1...|0bcbcc3e0a09b3f6a...|           5|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data:\n",
    "df = spark.read.parquet('C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\')\n",
    "\n",
    "# Clean data:\n",
    "df = df.na.drop()\n",
    "\n",
    "# Select important features from data\n",
    "columns = [\n",
    "    \"review_id\",\n",
    "    \"order_id\",\n",
    "    \"review_score\"\n",
    "]\n",
    "\n",
    "reviews = df.select(*columns)\n",
    "\n",
    "(\n",
    "    reviews\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\reviews\\\\\")\n",
    ")\n",
    "\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79572cd8",
   "metadata": {},
   "source": [
    "## Order Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e898220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"teste\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .options(header='true', inferSchema='true', delimiter=\",\")\n",
    "    .load(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\olist_orders_dataset.csv\")\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "(\n",
    "    df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e5f80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|            order_id|         customer_id|order_status|\n",
      "+--------------------+--------------------+------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|\n",
      "|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|\n",
      "|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|   delivered|\n",
      "|949d5b44dbf5de918...|f88197465ea7920ad...|   delivered|\n",
      "|ad21c59c0840e6cb8...|8ab97904e6daea886...|   delivered|\n",
      "|a4591c265e18cb1dc...|503740e9ca751ccdd...|   delivered|\n",
      "|6514b8ad8028c9f2c...|9bdf08b4b3b52b552...|   delivered|\n",
      "|76c6e866289321a7c...|f54a9f0e6b351c431...|   delivered|\n",
      "|e69bfb5eb88e0ed6a...|31ad1d1b63eb99624...|   delivered|\n",
      "|e6ce16cb79ec1d90b...|494dded5b201313c6...|   delivered|\n",
      "|34513ce0c4fab462a...|7711cf624183d843a...|   delivered|\n",
      "|82566a660a982b15f...|d3e3b74c766bc6214...|   delivered|\n",
      "|5ff96c15d0b717ac6...|19402a48fe860416a...|   delivered|\n",
      "|432aaf21d85167c2c...|3df704f53d3f1d481...|   delivered|\n",
      "|dcb36b511fcac050b...|3b6828a50ffe54694...|   delivered|\n",
      "|403b97836b0c04a62...|738b086814c6fcc74...|   delivered|\n",
      "|116f0b09343b49556...|3187789bec9909876...|   delivered|\n",
      "|85ce859fd6dc634de...|059f7fc5719c7da6c...|   delivered|\n",
      "|83018ec114eee8641...|7f8c8b9c2ae27bf33...|   delivered|\n",
      "|203096f03d82e0dff...|d2b091571da224a1b...|   delivered|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data:\n",
    "df = spark.read.parquet('C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\parquet\\\\')\n",
    "\n",
    "# Clean data:\n",
    "df = df.na.drop()\n",
    "\n",
    "# Select important features from data\n",
    "columns = [\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"order_status\"\n",
    "]\n",
    "\n",
    "dataset = df.select(*columns)\n",
    "\n",
    "(\n",
    "    dataset\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\dataset\\\\\")\n",
    ")\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd09a11",
   "metadata": {},
   "source": [
    "## Agregação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eacb1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- price_total: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_value: double (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_score: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = spark.read.parquet(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\items\\\\\")\n",
    "payments = spark.read.parquet(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\payments\\\\\")\n",
    "customers = spark.read.parquet(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\customers\\\\\")\n",
    "reviews = spark.read.parquet(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\reviews\\\\\")\n",
    "dataset = spark.read.parquet(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\dataset\\\\\")\n",
    "\n",
    "agg1 = items.join(payments , on=['order_id'] , how = 'left')\n",
    "agg2 = agg1.join(reviews , on=['order_id'] , how = 'left')\n",
    "agg3 = agg2.join(dataset, on=['order_id'] , how = 'left')\n",
    "agg = agg3.join(customers , on=['customer_id'] , how = 'left')\n",
    "\n",
    "agg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a0615ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+-------------+------------------+------------+-------------+--------------------+------------+------------+--------------------+------------------------+--------------+\n",
      "|         customer_id|            order_id|          product_id| price|freight_value|       price_total|payment_type|payment_value|           review_id|review_score|order_status|  customer_unique_id|customer_zip_code_prefix|customer_state|\n",
      "+--------------------+--------------------+--------------------+------+-------------+------------------+------------+-------------+--------------------+------------+------------+--------------------+------------------------+--------------+\n",
      "|3ce436f183e68e078...|00010242fe8c5a6d1...|4244733e06e7ecb49...|  58.9|        13.29|             72.19| credit_card|        72.19|                null|        null|   delivered|871766c5855e863f6...|                   28013|            RJ|\n",
      "|f6dd3ec061db4e398...|00018f77f2f0320c5...|e5f2d52b802189ee6...| 239.9|        19.93|            259.83| credit_card|       259.83|                null|        null|   delivered|eb28e67c4c0b83846...|                   15775|            SP|\n",
      "|6489ae5e4333f3693...|000229ec398224ef6...|c777355d18b72b67a...| 199.0|        17.87|            216.87| credit_card|       216.87|                null|        null|   delivered|3818d81c6709e39d0...|                   35661|            MG|\n",
      "|d4eb9395c8c0431ee...|00024acbcdf0a6daa...|7634da152a4610f15...| 12.99|        12.79|             25.78| credit_card|        25.78|                null|        null|   delivered|af861d436cfc08b2c...|                   12952|            SP|\n",
      "|58dbd0b2d70206bf4...|00042b26cf59d7ce6...|ac6c3623068f30de0...| 199.9|        18.14|218.04000000000002| credit_card|       218.04|                null|        null|   delivered|64b576fb70d441e8f...|                   13226|            SP|\n",
      "|816cbea969fe5b689...|00048cc3ae777c65d...|ef92defde845ab845...|  21.9|        12.69|34.589999999999996|      boleto|        34.59|                null|        null|   delivered|85c835d128beae5b4...|                   38017|            MG|\n",
      "|32e2e6ab09e778d99...|00054e8431b9d7675...|8d4f2bb7e93e6710a...|  19.9|        11.85|             31.75| credit_card|        31.75|                null|        null|   delivered|635d9ac1680f03288...|                   16700|            SP|\n",
      "|9ed5e522dd9dd85b4...|000576fe39319847c...|557d850972a7d6f79...| 810.0|        70.75|            880.75| credit_card|       880.75|                null|        null|   delivered|fda4476abb6307ab3...|                   11702|            SP|\n",
      "|16150771dfd477626...|0005a1a1728c9d785...|310ae3c140ff94b03...|145.95|        11.65|             157.6| credit_card|        157.6|                null|        null|   delivered|639d23421f5517f69...|                   11075|            SP|\n",
      "|351d3cb2cee3c7fd0...|0005f50442cb953dc...|4535b0e1091c278df...| 53.99|         11.4|             65.39| credit_card|        65.39|                null|        null|   delivered|0782c41380992a5a5...|                    6636|            SP|\n",
      "|c6fc061d86fab1e2b...|00061f2a7bc09da83...|d63c1011f49d98b97...| 59.99|         8.88|             68.87| credit_card|        68.87|                null|        null|   delivered|107e6259485efac66...|                   13419|            SP|\n",
      "|6a899e55865de6549...|00063b381e2406b52...|f177554ea93259a5b...|  45.0|        12.98|57.980000000000004| credit_card|        57.98|                null|        null|   delivered|3fb97204945ca0c01...|                   15910|            SP|\n",
      "|5d178120c29c61748...|0006ec9db01a64e59...|99a4788cb24856965...|  74.0|        23.32|             97.32| credit_card|        97.32|6322c405c0f34bf3a...|           5|   delivered|7ed0ea20347f67fe6...|                   21810|            RJ|\n",
      "|2355af7c75e7c98b4...|0008288aa423d2a3f...|368c6c730842d7801...|  49.9|        13.37|63.269999999999996|      boleto|       126.54|                null|        null|   delivered|9e415999542497142...|                    6600|            SP|\n",
      "|2355af7c75e7c98b4...|0008288aa423d2a3f...|368c6c730842d7801...|  49.9|        13.37|63.269999999999996|      boleto|       126.54|                null|        null|   delivered|9e415999542497142...|                    6600|            SP|\n",
      "|2a30c97668e81df7c...|0009792311464db53...|8cab8abac59158715...|  99.9|        27.65|127.55000000000001|      boleto|       127.55|                null|        null|   delivered|4987996ddcd0ddb20...|                   37137|            MG|\n",
      "|8a250edc40ebc5c39...|0009c9a17f916a706...|3f27ac8e699df3d30...| 639.0|        11.34|            650.34| credit_card|       650.34|                null|        null|   delivered|6062db572f3ef38b7...|                   13044|            SP|\n",
      "|fff5169e583fd07fa...|000aed2e25dbad2f9...|4fa33915031a8cde0...| 144.0|         8.77|            152.77| credit_card|       152.77|                null|        null|   delivered|6457be0b331148fb5...|                   13458|            SP|\n",
      "|3773bcf1a6fbd2923...|000c3e6612759851c...|b50c950aba0dcead2...|  99.0|        13.71|112.71000000000001|      boleto|       112.71|                null|        null|   delivered|f5f088001070650f9...|                   13208|            SP|\n",
      "|2b01d668726fb0b75...|000e562887b1f2006...|5ed9eaf534f6936b5...|  25.0|        16.11|             41.11| credit_card|        41.11|                null|        null|   delivered|b2c72d1e9f6430603...|                   11533|            SP|\n",
      "+--------------------+--------------------+--------------------+------+-------------+------------------+------------+-------------+--------------------+------------+------------+--------------------+------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e93f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    agg\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\agg\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb9615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    agg\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"csv\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\agg\\\\\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9f8f9",
   "metadata": {},
   "source": [
    "## Recomendação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8fe3722",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o226.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 50) (host.docker.internal executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\r\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\r\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv; isDirectory=false; length=22618856; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\r\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\r\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\r\n\tat scala.util.Success.map(Try.scala:213)\r\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\r\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\r\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\r\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\r\nCaused by: java.lang.RuntimeException: file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv is not a Parquet file. Expected magic number at tail, but found [83, 80, 13, 10]\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:527)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:125)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:168)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\r\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\r\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv; isDirectory=false; length=22618856; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\r\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\r\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\r\n\tat scala.util.Success.map(Try.scala:213)\r\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\r\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\r\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\r\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\r\nCaused by: java.lang.RuntimeException: file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv is not a Parquet file. Expected magic number at tail, but found [83, 80, 13, 10]\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rec \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mGabriel\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mbackup\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mRepositorios\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mMBA_project\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43molist\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43marchive\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_score\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      8\u001b[0m rec \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    353\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    355\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[0;32m    356\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[0;32m    362\u001b[0m )\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o226.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 50) (host.docker.internal executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\r\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\r\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv; isDirectory=false; length=22618856; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\r\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\r\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\r\n\tat scala.util.Success.map(Try.scala:213)\r\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\r\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\r\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\r\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\r\nCaused by: java.lang.RuntimeException: file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv is not a Parquet file. Expected magic number at tail, but found [83, 80, 13, 10]\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:70)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:527)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:125)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:168)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)\r\n\tat scala.Option.orElse(Option.scala:447)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \r\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\r\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:375)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:477)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:523)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:517)\r\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:76)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv; isDirectory=false; length=22618856; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:864)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:490)\r\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\r\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\r\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\r\n\tat scala.util.Success.map(Try.scala:213)\r\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\r\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\r\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\r\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\r\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\r\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\r\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\r\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\r\nCaused by: java.lang.RuntimeException: file:/C:/Users/Gabriel/Desktop/backup/Repositorios/MBA_project/data/olist/archive/agg/part-00000-baeb1500-1f55-409d-835d-9db183fbabfa-c000.csv is not a Parquet file. Expected magic number at tail, but found [83, 80, 13, 10]\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:556)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:776)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:44)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:484)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "rec = spark.read.parquet(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\agg\\\\\")\n",
    "columns = [\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"review_score\"\n",
    "]\n",
    "\n",
    "rec = rec.select(*columns)\n",
    "\n",
    "(\n",
    "    rec\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .save(\"C:\\\\Users\\\\Gabriel\\\\Desktop\\\\backup\\\\Repositorios\\\\MBA_project\\\\data\\\\olist\\\\archive\\\\rec\\\\\")\n",
    ")\n",
    "\n",
    "rec.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
